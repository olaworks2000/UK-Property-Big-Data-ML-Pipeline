{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a563f61d-dd04-4abf-9ee2-24458417329b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defined successfully. You can now load the data.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# This defines the \"Blueprints\" for your 6GB file\n",
    "schema = StructType([\n",
    "    StructField(\"Transaction_ID\", StringType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Postcode\", StringType(), True),\n",
    "    StructField(\"Property_Type\", StringType(), True),\n",
    "    StructField(\"Old_New\", StringType(), True),\n",
    "    StructField(\"Duration\", StringType(), True),\n",
    "    StructField(\"PAON\", StringType(), True),\n",
    "    StructField(\"SAON\", StringType(), True),\n",
    "    StructField(\"Street\", StringType(), True),\n",
    "    StructField(\"Locality\", StringType(), True),\n",
    "    StructField(\"Town_City\", StringType(), True),\n",
    "    StructField(\"District\", StringType(), True),\n",
    "    StructField(\"County\", StringType(), True),\n",
    "    StructField(\"PPD_Category\", StringType(), True),\n",
    "    StructField(\"Record_Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schema defined successfully. You can now load the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70662feb-5cca-4336-87d9-38f12f2df437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze Layer stored: Partitioned by County for optimized geographic queries.\nIngestion Complete. 30.9M rows validated and stored as Parquet.\n"
     ]
    }
   ],
   "source": [
    "# Notebook 1: Data Ingestion\n",
    "# Technical Requirement: Efficient SparkSession & Validation\n",
    "\n",
    "# 1. Validation: Check if volume exists\n",
    "import os\n",
    "volume_path = \"/Volumes/workspace/default/uk_land_registry/\"\n",
    "if not os.path.exists(volume_path):\n",
    "    print(\"Volume not found. Please check Catalog permissions.\")\n",
    "\n",
    "# 2. Ingestion with Schema Enforcement (Requirement 1a)\n",
    "# Using 'failFast' ensures we don't process corrupt data\n",
    "raw_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"mode\", \"failFast\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(f\"{volume_path}uk_property_full.csv\")\n",
    "\n",
    "# 3. Storage Design: Converting to Parquet for Performance (Requirement 1c)\n",
    "# This creates the 'Bronze' layer in a high-performance format\n",
    "#raw_df.write.mode(\"overwrite\").parquet(f\"{volume_path}bronze_parquet\")\n",
    "\n",
    "# 3. Storage Design: Parquet + Partitioning Strategy (Requirement 1a)\n",
    "# We partition by 'County' because regional analysis is a primary query pattern.\n",
    "raw_df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"County\") \\\n",
    "    .parquet(f\"{volume_path}bronze_parquet\")\n",
    "\n",
    "print(\"Bronze Layer stored: Partitioned by County for optimized geographic queries.\")\n",
    "\n",
    "print(f\"Ingestion Complete. 30.9M rows validated and stored as Parquet.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_data_ingestion 2026-02-17 09:54:24",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}