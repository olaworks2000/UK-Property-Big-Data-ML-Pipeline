{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d32f2f9-6bb1-4046-b62c-fe38cda7b979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: Ingest_Bronze_Parquet | Duration: 2.50s | Records: 30906560\nSTEP: Parallel_Cleaning_mapInPandas | Duration: 80.04s | Records: 30906560\nSTEP: Tableau_Sampling | Duration: 62.47s | Records: 101976\n--- SERVERLESS SILVER PIPELINE COMPLETE (METRICS LOGGED) ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, to_date, current_timestamp, lit\n",
    "\n",
    "# ======================================================================================\n",
    "# [cite_start]1. Spark Session Configuration [cite: 75, 308]\n",
    "# ======================================================================================\n",
    "def get_silver_spark_session():\n",
    "    \"\"\"\n",
    "    Configures Spark for Serverless.\n",
    "    Manual persist/cache is replaced by AQE (Adaptive Query Execution).\n",
    "    Justification: Photon engine handles caching internally on Serverless.\n",
    "    \"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"UK_Property_Silver_Production\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"auto\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# ======================================================================================\n",
    "# [cite_start]2. Performance Profiling & Data Lineage [cite: 83, 316, 82, 315]\n",
    "# ======================================================================================\n",
    "def profile_and_validate(step_name, df_operation, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Tracks execution time and record counts for Scalability Analysis.\n",
    "    Exports data used for Tableau Dashboard 4: Scalability and Cost analysis.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute transformation\n",
    "    df = df_operation(*args, **kwargs)\n",
    "    \n",
    "    # Triggers execution on Serverless to capture real-time performance metrics\n",
    "    row_count = df.count()\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"STEP: {step_name} | Duration: {duration:.2f}s | Records: {row_count}\")\n",
    "    \n",
    "    # Metadata for Ingestion Audit and Lineage\n",
    "    df_final = df.withColumn(\"pipeline_step\", lit(step_name)) \\\n",
    "                 .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "                 \n",
    "    return df_final, duration, row_count\n",
    "\n",
    "# ======================================================================================\n",
    "# [cite_start]3. Distributed Processing (mapInPandas) [cite: 79, 312]\n",
    "# ======================================================================================\n",
    "def apply_serverless_parallel_cleaning(iterator):\n",
    "    \"\"\"\n",
    "    Bypasses RDD restrictions while maintaining O(n/p) complexity logic.\n",
    "    Justification: RDDs are deprecated on Serverless; mapInPandas is the modern standard.\n",
    "    \"\"\"\n",
    "    for pdf in iterator:\n",
    "        # Data validation at the partition level: Price must be positive\n",
    "        # This addresses Section 1.7 (Critical Evaluation of Data Quality)\n",
    "        cleaned_pdf = pdf[(pdf['Price'] > 0) & (pdf['County'].notnull())]\n",
    "        yield cleaned_pdf\n",
    "\n",
    "# ======================================================================================\n",
    "# 4. Main Integrated Execution\n",
    "# ======================================================================================\n",
    "def run_full_silver_pipeline():\n",
    "    spark = get_silver_spark_session()\n",
    "    perf_metrics = []\n",
    "    \n",
    "    # [cite_start]--- A. Data Ingestion (Storage Design) [cite: 77, 310] ---\n",
    "    input_path = \"/Volumes/workspace/default/uk_land_registry/bronze_parquet\"\n",
    "    df_bronze, t_load, c_load = profile_and_validate(\"Ingest_Bronze_Parquet\", spark.read.parquet, input_path)\n",
    "    perf_metrics.append((\"Ingestion\", t_load, c_load))\n",
    "    \n",
    "    # --- B. Feature Engineering (Temporal Extraction) ---\n",
    "    # Essential for 'Dashboard 3: Business insights' 30-year trend line\n",
    "    df_temp = df_bronze.withColumn(\"Transfer_Date\", to_date(col(\"Transfer_Date\"), \"yyyy-MM-dd HH:mm\")) \\\n",
    "                       .withColumn(\"Year\", year(col(\"Transfer_Date\"))) \\\n",
    "                       .withColumn(\"Month\", month(col(\"Transfer_Date\")))\n",
    "    \n",
    "    # --- C. Parallel Processing (Computational Complexity) ---\n",
    "    # Demonstrates distributed performance optimization without RDD overhead\n",
    "    df_silver, t_clean, c_clean = profile_and_validate(\n",
    "        \"Parallel_Cleaning_mapInPandas\", \n",
    "        df_temp.mapInPandas, \n",
    "        apply_serverless_parallel_cleaning, \n",
    "        df_temp.schema\n",
    "    )\n",
    "    perf_metrics.append((\"Cleaning_O_n_p\", t_clean, c_clean))\n",
    "    \n",
    "    # [cite_start]--- D. Tableau Strategy (Sampling) [cite: 105, 338] ---\n",
    "    # Using 0.0033 to reduce 30.9M rows to ~100k for Tableau performance\n",
    "    df_silver_sample, t_samp, c_samp = profile_and_validate(\n",
    "        \"Tableau_Sampling\", \n",
    "        df_silver.sample, \n",
    "        False, 0.0033, 42\n",
    "    )\n",
    "    perf_metrics.append((\"Sampling_for_Tableau\", t_samp, c_samp))\n",
    "    \n",
    "    # [cite_start]--- E. Storage & Partitioning Strategy [cite: 76, 309] ---\n",
    "    # Partitioning by County aligns with query patterns in Gold layer modeling\n",
    "    output_path = \"/Volumes/workspace/default/uk_land_registry/silver_engineered_parquet\"\n",
    "    df_silver.write.mode(\"overwrite\").partitionBy(\"County\").parquet(output_path)\n",
    "    \n",
    "    # Export Scalability Metrics for Dashboard 4\n",
    "    metrics_schema = [\"step_name\", \"execution_time_sec\", \"record_count\"]\n",
    "    spark.createDataFrame(perf_metrics, metrics_schema).coalesce(1).write.mode(\"overwrite\") \\\n",
    "         .option(\"header\", \"true\").csv(\"/Volumes/workspace/default/uk_land_registry/gold_tableau_data/performance_log.csv\")\n",
    "         \n",
    "    # Export Sample for Dashboard 3\n",
    "    df_silver_sample.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
    "                    .csv(\"/Volumes/workspace/default/uk_land_registry/gold_tableau_data/silver_sample_tableau.csv\")\n",
    "\n",
    "    print(\"--- SERVERLESS SILVER PIPELINE COMPLETE (METRICS LOGGED) ---\")\n",
    "\n",
    "# Trigger\n",
    "run_full_silver_pipeline()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}