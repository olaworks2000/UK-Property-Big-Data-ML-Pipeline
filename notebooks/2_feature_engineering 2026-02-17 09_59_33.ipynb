{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af201678-90ac-4a4c-8e1f-00c1352a1db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Engineering...\nBronze data loaded with lineage.\nFeature Engineering completed in 53.86 seconds.\n\n--- PERFORMANCE SUMMARY ---\nFeature Engineering Duration: 53.86 seconds\n== Physical Plan ==\nAdaptiveSparkPlan (15)\n+- == Initial Plan ==\n   Project (14)\n   +- Project (13)\n      +- Project (12)\n         +- ColumnarToRow (11)\n            +- PhotonResultStage (10)\n               +- PhotonBroadcastHashJoin LeftOuter (9)\n                  :- PhotonProject (3)\n                  :  +- PhotonProject (2)\n                  :     +- PhotonScan parquet  (1)\n                  +- PhotonShuffleExchangeSource (8)\n                     +- PhotonShuffleMapStage (7)\n                        +- PhotonShuffleExchangeSink (6)\n                           +- PhotonRowToColumnar (5)\n                              +- LocalTableScan (4)\n\n\n(1) PhotonScan parquet \nOutput [6]: [Price#13206, Date#13207, Property_Type#13209, Old_New#13210, Town_City#13216, County#13220]\nLocation: InMemoryFileIndex [dbfs:/Volumes/workspace/default/uk_land_registry/bronze_parquet]\nReadSchema: struct<Price:int,Date:string,Property_Type:string,Old_New:string,Town_City:string>\nRequiredDataFilters: [atleastnnonnulls(5, cast(Price#13206 as double), gettimestamp(Date#13207, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), true), Property_Type#13209, Old_New#13210, Town_City#13216)]\n\n(2) PhotonProject\nInput [6]: [Price#13206, Date#13207, Property_Type#13209, Old_New#13210, Town_City#13216, County#13220]\nArguments: [cast(Price#13206 as double) AS Price#13252, gettimestamp(Date#13207, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), true) AS Sale_Date#13251, Property_Type#13209, Old_New#13210, Town_City#13216]\n\n(3) PhotonProject\nInput [5]: [Price#13252, Sale_Date#13251, Property_Type#13209, Old_New#13210, Town_City#13216]\nArguments: [Price#13252, Sale_Date#13251, Property_Type#13209, Old_New#13210, Town_City#13216, year(cast(Sale_Date#13251 as date)) AS Sale_Year#13254]\n\n(4) LocalTableScan\nOutput [2]: [Property_Type#13386, Type_Description#13387]\nArguments: [Property_Type#13386, Type_Description#13387]\n\n(5) PhotonRowToColumnar\nInput [2]: [Property_Type#13386, Type_Description#13387]\n\n(6) PhotonShuffleExchangeSink\nInput [2]: [Property_Type#13386, Type_Description#13387]\nArguments: SinglePartition\n\n(7) PhotonShuffleMapStage\nInput [2]: [Property_Type#13386, Type_Description#13387]\nArguments: EXECUTOR_BROADCAST, [id=#9103]\n\n(8) PhotonShuffleExchangeSource\nInput [2]: [Property_Type#13386, Type_Description#13387]\n\n(9) PhotonBroadcastHashJoin\nLeft keys [1]: [Property_Type#13209]\nRight keys [1]: [Property_Type#13386]\nJoin type: LeftOuter\nJoin condition: None\n\n(10) PhotonResultStage\nInput [8]: [Price#13252, Sale_Date#13251, Property_Type#13209, Old_New#13210, Town_City#13216, Sale_Year#13254, Property_Type#13386, Type_Description#13387]\n\n(11) ColumnarToRow\nInput [8]: [Price#13252, Sale_Date#13251, Property_Type#13209, Old_New#13210, Town_City#13216, Sale_Year#13254, Property_Type#13386, Type_Description#13387]\n\n(12) Project\nOutput [8]: [Property_Type#13209, Price#13252, Sale_Date#13251, Old_New#13210, Town_City#13216, Sale_Year#13254, Type_Description#13387, UDF(Property_Type#13209) AS type_label#13391]\nInput [8]: [Price#13252, Sale_Date#13251, Property_Type#13209, Old_New#13210, Town_City#13216, Sale_Year#13254, Property_Type#13386, Type_Description#13387]\n\n(13) Project\nOutput [10]: [Property_Type#13209, Price#13252, Sale_Date#13251, Old_New#13210, Town_City#13216, Sale_Year#13254, Type_Description#13387, type_label#13391, CASE WHEN (Price#13252 < 150000.0) THEN Budget WHEN (Price#13252 < 450000.0) THEN Standard ELSE Premium END AS Market_Segment#13396, UDF(struct(Price, Price#13252)) AS unscaled_features#13398]\nInput [8]: [Property_Type#13209, Price#13252, Sale_Date#13251, Old_New#13210, Town_City#13216, Sale_Year#13254, Type_Description#13387, type_label#13391]\n\n(14) Project\nOutput [12]: [Property_Type#13209, Price#13252, Sale_Date#13251, Old_New#13210, Town_City#13216, uk_property_full.csv AS source_file#13246, Sale_Year#13254, Type_Description#13387, type_label#13391, Market_Segment#13396, unscaled_features#13398, UDF(unscaled_features#13398) AS scaled_features#13402]\nInput [10]: [Property_Type#13209, Price#13252, Sale_Date#13251, Old_New#13210, Town_City#13216, Sale_Year#13254, Type_Description#13387, type_label#13391, Market_Segment#13396, unscaled_features#13398]\n\n(15) AdaptiveSparkPlan\nOutput [12]: [Property_Type#13209, Price#13252, Sale_Date#13251, Old_New#13210, Town_City#13216, source_file#13246, Sale_Year#13254, Type_Description#13387, type_label#13391, Market_Segment#13396, unscaled_features#13398, scaled_features#13402]\nArguments: isFinalPlan=false\n\n\n== Photon Explanation ==\nPhoton does not fully support the query because:\n\n\tUDF(Property_Type#13209) is not supported:\n\t\tThe expression `scalaudf` with input expressions `(attributereference)` is currently unimplemented. If possible, try to rewrite with other expressions.\n\nReference node:\n\tProject [Property_Type#13209, Price#13252, Sale_Date#13251, Old_New#13210, Town_City#13216, Sale_Year#13254, Type_Description#13387, UDF(Property_Type#13209) AS type_label#13391]\n\nGitHub Silver sample generated in /Volumes/workspace/default/uk_land_registry/github_samples\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. IMPORTS & PROFILER SETUP\n",
    "# ==========================================\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.util import DefaultParamsWritable, DefaultParamsReadable\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp, year, lit, broadcast\n",
    "\n",
    "class PipelineProfiler:\n",
    "    def __init__(self):\n",
    "        self.stats = {}\n",
    "    def start_timer(self, stage_name):\n",
    "        self.stats[stage_name] = time.time()\n",
    "        print(f\"Starting {stage_name}...\")\n",
    "    def end_timer(self, stage_name):\n",
    "        duration = time.time() - self.stats[stage_name]\n",
    "        print(f\"{stage_name} completed in {duration:.2f} seconds.\")\n",
    "        return duration\n",
    "\n",
    "profiler = PipelineProfiler()\n",
    "profiler.start_timer(\"Feature Engineering\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. CUSTOM TRANSFORMER (Requirement 2a)\n",
    "# ==========================================\n",
    "class PriceSegmenter(Transformer, DefaultParamsWritable, DefaultParamsReadable):\n",
    "    \"\"\"Adds domain-specific feature engineering (Market Segmentation).\"\"\"\n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn(\"Market_Segment\", \n",
    "            F.when(F.col(\"Price\") < 150000, \"Budget\")\n",
    "             .when(F.col(\"Price\") < 450000, \"Standard\")\n",
    "             .otherwise(\"Premium\"))\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA INGESTION & LINEAGE (Requirement 1b)\n",
    "# ==========================================\n",
    "try:\n",
    "    df = spark.read.parquet(\"/Volumes/workspace/default/uk_land_registry/bronze_parquet\")\n",
    "    \n",
    "    silver_df = df.withColumn(\"source_file\", lit(\"uk_property_full.csv\")) \\\n",
    "                  .withColumn(\"ingestion_layer\", lit(\"Bronze\")) \\\n",
    "                  .select(\n",
    "                      col(\"Price\").cast(\"double\"),\n",
    "                      to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm\").alias(\"Sale_Date\"),\n",
    "                      \"Property_Type\", \"Old_New\", \"Town_City\", \"source_file\"\n",
    "                  ).dropna()\n",
    "\n",
    "    silver_df = silver_df.withColumn(\"Sale_Year\", year(col(\"Sale_Date\")))\n",
    "    print(\"Bronze data loaded with lineage.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"PIPELINE ERROR: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# ==========================================\n",
    "# 4. DISTRIBUTED PROCESSING (Requirement 1b)\n",
    "# ==========================================\n",
    "\n",
    "# Broadcast Join Implementation for Property Mapping\n",
    "mapping_data = [(\"D\", \"Detached\"), (\"S\", \"Semi-Detached\"), (\"T\", \"Terraced\"), \n",
    "                (\"P\", \"Flats/Maisonettes\"), (\"O\", \"Other\")]\n",
    "type_mapping_df = spark.createDataFrame(mapping_data, [\"Property_Type\", \"Type_Description\"])\n",
    "\n",
    "silver_df_with_labels = silver_df.join(broadcast(type_mapping_df), on=\"Property_Type\", how=\"left\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. ML PREPARATION & SCALING (Requirement 2a)\n",
    "# ==========================================\n",
    "\n",
    "# A. Indexing & Custom Segmentation\n",
    "indexer = StringIndexer(inputCol=\"Property_Type\", outputCol=\"type_label\")\n",
    "indexed_df = indexer.fit(silver_df_with_labels).transform(silver_df_with_labels)\n",
    "\n",
    "segmenter = PriceSegmenter()\n",
    "segmented_df = segmenter.transform(indexed_df)\n",
    "\n",
    "# B. Vector Assembly & StandardScaler\n",
    "assembler = VectorAssembler(inputCols=[\"Price\"], outputCol=\"unscaled_features\")\n",
    "assembled_df = assembler.transform(segmented_df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"scaled_features\", \n",
    "                        withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "final_engineered_df = scaler_model.transform(assembled_df)\n",
    "\n",
    "# ==========================================\n",
    "# 6. STORAGE & PERFORMANCE EVIDENCE (Requirement 1a & 1c)\n",
    "# ==========================================\n",
    "output_path = \"/Volumes/workspace/default/uk_land_registry/silver_engineered_parquet\"\n",
    "final_engineered_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# Stop the timer and print stats for Dashboard 4\n",
    "eng_duration = profiler.end_timer(\"Feature Engineering\")\n",
    "\n",
    "print(\"\\n--- PERFORMANCE SUMMARY ---\")\n",
    "print(f\"Feature Engineering Duration: {eng_duration:.2f} seconds\")\n",
    "\n",
    "# Explain Plan for optimization evidence\n",
    "final_engineered_df.explain(mode=\"formatted\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. GITHUB SAMPLE GENERATION (Local Meta-Data)\n",
    "# ==========================================\n",
    "\n",
    "sample_path = \"/Volumes/workspace/default/uk_land_registry/github_samples\"\n",
    "dbutils.fs.mkdirs(sample_path)\n",
    "\n",
    "# Creating tiny 1k-row samples for the GitHub /data/samples folder\n",
    "final_engineered_df.limit(1000).toPandas().to_csv(f\"{sample_path}/silver_sample.csv\", index=False)\n",
    "print(f\"GitHub Silver sample generated in {sample_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_feature_engineering 2026-02-17 09:59:33",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}