{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df614447-0c3a-4c72-9a71-c2f6c1a4f771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Engineering...\n",
      "Bronze data loaded with lineage.\n",
      "Exporting clean 100k sample for Tableau...\n",
      "Feature Engineering completed in 88.02 seconds.\n",
      "--- Notebook 2 Complete: 100k sample generated ---\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. IMPORTS & PROFILER SETUP\n",
    "# ==========================================\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.util import DefaultParamsWritable, DefaultParamsReadable\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp, year, lit, broadcast\n",
    "\n",
    "class PipelineProfiler:\n",
    "    def __init__(self):\n",
    "        self.stats = {}\n",
    "    def start_timer(self, stage_name):\n",
    "        self.stats[stage_name] = time.time()\n",
    "        print(f\"Starting {stage_name}...\")\n",
    "    def end_timer(self, stage_name):\n",
    "        duration = time.time() - self.stats[stage_name]\n",
    "        print(f\"{stage_name} completed in {duration:.2f} seconds.\")\n",
    "        return duration\n",
    "\n",
    "profiler = PipelineProfiler()\n",
    "profiler.start_timer(\"Feature Engineering\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. CUSTOM TRANSFORMER (Requirement 2a)\n",
    "# ==========================================\n",
    "class PriceSegmenter(Transformer, DefaultParamsWritable, DefaultParamsReadable):\n",
    "    \"\"\"Requirement 2a: Adds domain-specific Market Segmentation.\"\"\"\n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn(\"Market_Segment\", \n",
    "            F.when(F.col(\"Price\") < 150000, \"Budget\")\n",
    "             .when(F.col(\"Price\") < 450000, \"Standard\")\n",
    "             .otherwise(\"Premium\"))\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA INGESTION & LINEAGE (Requirement 1b)\n",
    "# ==========================================\n",
    "try:\n",
    "    df = spark.read.parquet(\"/Volumes/workspace/default/uk_land_registry/bronze_parquet\")\n",
    "    \n",
    "    silver_df = df.withColumn(\"source_file\", lit(\"uk_property_full.csv\")) \\\n",
    "                  .withColumn(\"ingestion_layer\", lit(\"Bronze\")) \\\n",
    "                  .select(\n",
    "                      col(\"Price\").cast(\"double\"),\n",
    "                      to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm\").alias(\"Sale_Date\"),\n",
    "                      \"Property_Type\", \"Old_New\", \"Town_City\", \"source_file\"\n",
    "                  ).dropna()\n",
    "\n",
    "    silver_df = silver_df.withColumn(\"Sale_Year\", year(col(\"Sale_Date\")))\n",
    "    print(\"Bronze data loaded with lineage.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"PIPELINE ERROR: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# ==========================================\n",
    "# 4. DISTRIBUTED PROCESSING (Requirement 1b)\n",
    "# ==========================================\n",
    "mapping_data = [(\"D\", \"Detached\"), (\"S\", \"Semi-Detached\"), (\"T\", \"Terraced\"), \n",
    "                (\"P\", \"Flats/Maisonettes\"), (\"O\", \"Other\")]\n",
    "type_mapping_df = spark.createDataFrame(mapping_data, [\"Property_Type\", \"Type_Description\"])\n",
    "\n",
    "silver_df_with_labels = silver_df.join(broadcast(type_mapping_df), on=\"Property_Type\", how=\"left\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. ML PREPARATION & GEOGRAPHIC ENCODING\n",
    "# ==========================================\n",
    "# Target Indexing\n",
    "type_indexer = StringIndexer(inputCol=\"Property_Type\", outputCol=\"type_label\")\n",
    "indexed_df = type_indexer.fit(silver_df_with_labels).transform(silver_df_with_labels)\n",
    "\n",
    "# NEW: Geographic Indexing (Requirement 2a)\n",
    "city_indexer = StringIndexer(inputCol=\"Town_City\", outputCol=\"city_label\", handleInvalid=\"skip\")\n",
    "indexed_df = city_indexer.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "# Custom Segmentation\n",
    "segmented_df = PriceSegmenter().transform(indexed_df)\n",
    "\n",
    "# Scaling Logic\n",
    "price_assembler = VectorAssembler(inputCols=[\"Price\"], outputCol=\"unscaled_price\")\n",
    "price_assembled_df = price_assembler.transform(segmented_df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"unscaled_price\", outputCol=\"scaled_features\", \n",
    "                        withStd=True, withMean=True)\n",
    "scaled_df = scaler.fit(price_assembled_df).transform(price_assembled_df)\n",
    "\n",
    "# FINAL VECTOR: Combines Scaled Price + City Label\n",
    "final_assembler = VectorAssembler(inputCols=[\"scaled_features\", \"city_label\"], outputCol=\"final_features\")\n",
    "final_engineered_df = final_assembler.transform(scaled_df)\n",
    "\n",
    "# ==========================================\n",
    "# 6. STORAGE (Requirement 1a & 1c)\n",
    "# ==========================================\n",
    "# Save to Parquet (Supports Vectors) for Notebooks 3 & 4\n",
    "output_path = \"/Volumes/workspace/default/uk_land_registry/silver_engineered_parquet\"\n",
    "final_engineered_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# ==========================================\n",
    "# 7. EXPORT FIXED 100K SAMPLE (For Tableau)\n",
    "# ==========================================\n",
    "# We cast Vectors to Strings to avoid [UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] error\n",
    "print(\"Exporting clean 100k sample for Tableau...\")\n",
    "tableau_export_df = final_engineered_df.select(\n",
    "    \"*\",\n",
    "    col(\"unscaled_price\").cast(\"string\").alias(\"unscaled_price_str\"),\n",
    "    col(\"scaled_features\").cast(\"string\").alias(\"scaled_features_str\"),\n",
    "    col(\"final_features\").cast(\"string\").alias(\"final_features_str\")\n",
    ").drop(\"unscaled_price\", \"scaled_features\", \"final_features\")\n",
    "\n",
    "# coalesce(1) ensures we get exactly one 100,000 row CSV file\n",
    "tableau_export_df.limit(100000).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
    "    .csv(\"/Volumes/workspace/default/uk_land_registry/gold_tableau_data\")\n",
    "\n",
    "# GITHUB SAMPLE (1,000 rows)\n",
    "sample_path = \"/Volumes/workspace/default/uk_land_registry/github_samples\"\n",
    "tableau_export_df.limit(1000).toPandas().to_csv(f\"{sample_path}/silver_sample.csv\", index=False)\n",
    "\n",
    "eng_duration = profiler.end_timer(\"Feature Engineering\")\n",
    "print(f\"--- Notebook 2 Complete: 100k sample generated ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_feature_engineering 2026-02-17 09:59:33",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
