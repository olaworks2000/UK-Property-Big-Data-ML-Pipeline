{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df4a358-d6dd-4855-817d-c4a75d36b8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 2 Complete: Silver Layer stored with Scaling and Feature Engineering applied.\n+--------+--------------------+----------+\n|   Price|     scaled_features|type_label|\n+--------+--------------------+----------+\n|156000.0|[-0.0795096213290...|       3.0|\n| 82500.0|[-0.1544214218193...|       2.0|\n|165000.0|[-0.0703367477996...|       1.0|\n|205000.0|[-0.0295684210021...|       1.0|\n|117000.0|[-0.1192587399565...|       0.0|\n+--------+--------------------+----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, year, month\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "\n",
    "# 1. Load the Parquet data we just created\n",
    "df = spark.read.parquet(\"/Volumes/workspace/default/uk_land_registry/bronze_parquet\")\n",
    "\n",
    "# 2. Cleaning & Temporal Considerations (Requirement 4a)\n",
    "# We handle missing values and extract the Year to help with 'Temporal' analysis\n",
    "silver_df = df.select(\n",
    "    col(\"Price\").cast(\"double\"), # Casting to double for the Scaler later\n",
    "    to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm\").alias(\"Sale_Date\"),\n",
    "    col(\"Property_Type\"),\n",
    "    col(\"Old_New\"),\n",
    "    col(\"Town_City\")\n",
    ").dropna()\n",
    "\n",
    "silver_df = silver_df.withColumn(\"Sale_Year\", year(col(\"Sale_Date\")))\n",
    "\n",
    "# 3. Feature Engineering (Requirement 2a)\n",
    "# Converting Categorical 'Property_Type' to numeric\n",
    "indexer = StringIndexer(inputCol=\"Property_Type\", outputCol=\"type_label\")\n",
    "indexed_df = indexer.fit(silver_df).transform(silver_df)\n",
    "\n",
    "# 4. Normalization/Scaling (Requirement 2a - 'Scaling/Normalization')\n",
    "# We assemble features then use StandardScaler so the 'Price' doesn't bias the model\n",
    "assembler = VectorAssembler(inputCols=[\"Price\"], outputCol=\"unscaled_features\")\n",
    "assembled_df = assembler.transform(indexed_df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "final_engineered_df = scaler_model.transform(assembled_df)\n",
    "\n",
    "# 5. Save the Silver Layer\n",
    "final_engineered_df.write.mode(\"overwrite\").parquet(\"/Volumes/workspace/default/uk_land_registry/silver_engineered_parquet\")\n",
    "\n",
    "print(f\"Notebook 2 Complete: Silver Layer stored with Scaling and Feature Engineering applied.\")\n",
    "final_engineered_df.select(\"Price\", \"scaled_features\", \"type_label\").show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_feature_engineering 2026-02-17 09:59:33",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}