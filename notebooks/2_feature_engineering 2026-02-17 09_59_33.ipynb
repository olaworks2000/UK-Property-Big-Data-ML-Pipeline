{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4bbb1b0-ae5b-410d-8a61-08359d1a79e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Engineering...\nBronze data loaded with lineage.\nFeature Engineering completed in 57.09 seconds.\n\n--- PERFORMANCE SUMMARY ---\nFeature Engineering Duration: 57.09 seconds\n== Physical Plan ==\nAdaptiveSparkPlan (18)\n+- == Initial Plan ==\n   Project (17)\n   +- Project (16)\n      +- Project (15)\n         +- Project (14)\n            +- Filter (13)\n               +- Project (12)\n                  +- ColumnarToRow (11)\n                     +- PhotonResultStage (10)\n                        +- PhotonBroadcastHashJoin LeftOuter (9)\n                           :- PhotonProject (3)\n                           :  +- PhotonProject (2)\n                           :     +- PhotonScan parquet  (1)\n                           +- PhotonShuffleExchangeSource (8)\n                              +- PhotonShuffleMapStage (7)\n                                 +- PhotonShuffleExchangeSink (6)\n                                    +- PhotonRowToColumnar (5)\n                                       +- LocalTableScan (4)\n\n\n(1) PhotonScan parquet \nOutput [6]: [Price#14146, Date#14147, Property_Type#14149, Old_New#14150, Town_City#14156, County#14160]\nLocation: InMemoryFileIndex [dbfs:/Volumes/workspace/default/uk_land_registry/bronze_parquet]\nReadSchema: struct<Price:int,Date:string,Property_Type:string,Old_New:string,Town_City:string>\nRequiredDataFilters: [atleastnnonnulls(5, cast(Price#14146 as double), gettimestamp(Date#14147, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), true), Property_Type#14149, Old_New#14150, Town_City#14156)]\n\n(2) PhotonProject\nInput [6]: [Price#14146, Date#14147, Property_Type#14149, Old_New#14150, Town_City#14156, County#14160]\nArguments: [cast(Price#14146 as double) AS Price#14406, gettimestamp(Date#14147, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), true) AS Sale_Date#14405, Property_Type#14149, Old_New#14150, Town_City#14156]\n\n(3) PhotonProject\nInput [5]: [Price#14406, Sale_Date#14405, Property_Type#14149, Old_New#14150, Town_City#14156]\nArguments: [Price#14406, Sale_Date#14405, Property_Type#14149, Old_New#14150, Town_City#14156, year(cast(Sale_Date#14405 as date)) AS Sale_Year#14408]\n\n(4) LocalTableScan\nOutput [2]: [Property_Type#14417, Type_Description#14418]\nArguments: [Property_Type#14417, Type_Description#14418]\n\n(5) PhotonRowToColumnar\nInput [2]: [Property_Type#14417, Type_Description#14418]\n\n(6) PhotonShuffleExchangeSink\nInput [2]: [Property_Type#14417, Type_Description#14418]\nArguments: SinglePartition\n\n(7) PhotonShuffleMapStage\nInput [2]: [Property_Type#14417, Type_Description#14418]\nArguments: EXECUTOR_BROADCAST, [id=#11830]\n\n(8) PhotonShuffleExchangeSource\nInput [2]: [Property_Type#14417, Type_Description#14418]\n\n(9) PhotonBroadcastHashJoin\nLeft keys [1]: [Property_Type#14149]\nRight keys [1]: [Property_Type#14417]\nJoin type: LeftOuter\nJoin condition: None\n\n(10) PhotonResultStage\nInput [8]: [Price#14406, Sale_Date#14405, Property_Type#14149, Old_New#14150, Town_City#14156, Sale_Year#14408, Property_Type#14417, Type_Description#14418]\n\n(11) ColumnarToRow\nInput [8]: [Price#14406, Sale_Date#14405, Property_Type#14149, Old_New#14150, Town_City#14156, Sale_Year#14408, Property_Type#14417, Type_Description#14418]\n\n(12) Project\nOutput [8]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, UDF(Property_Type#14149) AS type_label#14422]\nInput [8]: [Price#14406, Sale_Date#14405, Property_Type#14149, Old_New#14150, Town_City#14156, Sale_Year#14408, Property_Type#14417, Type_Description#14418]\n\n(13) Filter\nInput [8]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, type_label#14422]\nCondition : (atleastnnonnulls(1, Town_City#14156) AND UDF(Town_City#14156))\n\n(14) Project\nOutput [9]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, type_label#14422, UDF(Town_City#14156) AS city_label#14434]\nInput [8]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, type_label#14422]\n\n(15) Project\nOutput [11]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, type_label#14422, city_label#14434, CASE WHEN (Price#14406 < 150000.0) THEN Budget WHEN (Price#14406 < 450000.0) THEN Standard ELSE Premium END AS Market_Segment#14439, UDF(struct(Price, Price#14406)) AS unscaled_price#14441]\nInput [9]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, type_label#14422, city_label#14434]\n\n(16) Project\nOutput [12]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, type_label#14422, city_label#14434, Market_Segment#14439, unscaled_price#14441, UDF(unscaled_price#14441) AS scaled_features#14445]\nInput [11]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, type_label#14422, city_label#14434, Market_Segment#14439, unscaled_price#14441]\n\n(17) Project\nOutput [14]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, uk_property_full.csv AS source_file#14402, Sale_Year#14408, Type_Description#14418, type_label#14422, city_label#14434, Market_Segment#14439, unscaled_price#14441, scaled_features#14445, UDF(struct(scaled_features, scaled_features#14445, city_label, city_label#14434)) AS final_features#14450]\nInput [12]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, type_label#14422, city_label#14434, Market_Segment#14439, unscaled_price#14441, scaled_features#14445]\n\n(18) AdaptiveSparkPlan\nOutput [14]: [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, source_file#14402, Sale_Year#14408, Type_Description#14418, type_label#14422, city_label#14434, Market_Segment#14439, unscaled_price#14441, scaled_features#14445, final_features#14450]\nArguments: isFinalPlan=false\n\n\n== Photon Explanation ==\nPhoton does not fully support the query because:\n\n\tUDF(Property_Type#14149) is not supported:\n\t\tThe expression `scalaudf` with input expressions `(attributereference)` is currently unimplemented. If possible, try to rewrite with other expressions.\n\nReference node:\n\tProject [Property_Type#14149, Price#14406, Sale_Date#14405, Old_New#14150, Town_City#14156, Sale_Year#14408, Type_Description#14418, UDF(Property_Type#14149) AS type_label#14422]\n\nGitHub Silver sample generated in /Volumes/workspace/default/uk_land_registry/github_samples\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. IMPORTS & PROFILER SETUP\n",
    "# ==========================================\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.util import DefaultParamsWritable, DefaultParamsReadable\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp, year, lit, broadcast\n",
    "\n",
    "class PipelineProfiler:\n",
    "    def __init__(self):\n",
    "        self.stats = {}\n",
    "    def start_timer(self, stage_name):\n",
    "        self.stats[stage_name] = time.time()\n",
    "        print(f\"Starting {stage_name}...\")\n",
    "    def end_timer(self, stage_name):\n",
    "        duration = time.time() - self.stats[stage_name]\n",
    "        print(f\"{stage_name} completed in {duration:.2f} seconds.\")\n",
    "        return duration\n",
    "\n",
    "profiler = PipelineProfiler()\n",
    "profiler.start_timer(\"Feature Engineering\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. CUSTOM TRANSFORMER (Requirement 2a)\n",
    "# ==========================================\n",
    "class PriceSegmenter(Transformer, DefaultParamsWritable, DefaultParamsReadable):\n",
    "    \"\"\"Adds domain-specific feature engineering (Market Segmentation).\"\"\"\n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn(\"Market_Segment\", \n",
    "            F.when(F.col(\"Price\") < 150000, \"Budget\")\n",
    "             .when(F.col(\"Price\") < 450000, \"Standard\")\n",
    "             .otherwise(\"Premium\"))\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA INGESTION & LINEAGE (Requirement 1b)\n",
    "# ==========================================\n",
    "try:\n",
    "    df = spark.read.parquet(\"/Volumes/workspace/default/uk_land_registry/bronze_parquet\")\n",
    "    \n",
    "    silver_df = df.withColumn(\"source_file\", lit(\"uk_property_full.csv\")) \\\n",
    "                  .withColumn(\"ingestion_layer\", lit(\"Bronze\")) \\\n",
    "                  .select(\n",
    "                      col(\"Price\").cast(\"double\"),\n",
    "                      to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm\").alias(\"Sale_Date\"),\n",
    "                      \"Property_Type\", \"Old_New\", \"Town_City\", \"source_file\"\n",
    "                  ).dropna()\n",
    "\n",
    "    silver_df = silver_df.withColumn(\"Sale_Year\", year(col(\"Sale_Date\")))\n",
    "    print(\"Bronze data loaded with lineage.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"PIPELINE ERROR: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# ==========================================\n",
    "# 4. DISTRIBUTED PROCESSING (Requirement 1b)\n",
    "# ==========================================\n",
    "\n",
    "# Broadcast Join Implementation for Property Mapping\n",
    "mapping_data = [(\"D\", \"Detached\"), (\"S\", \"Semi-Detached\"), (\"T\", \"Terraced\"), \n",
    "                (\"P\", \"Flats/Maisonettes\"), (\"O\", \"Other\")]\n",
    "type_mapping_df = spark.createDataFrame(mapping_data, [\"Property_Type\", \"Type_Description\"])\n",
    "\n",
    "silver_df_with_labels = silver_df.join(broadcast(type_mapping_df), on=\"Property_Type\", how=\"left\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. ML PREPARATION & SCALING (Requirement 2a)\n",
    "# ==========================================\n",
    "\n",
    "# A. Indexing (Property Type & Geographic Town_City)\n",
    "# Indexing Property_Type for the target label\n",
    "type_indexer = StringIndexer(inputCol=\"Property_Type\", outputCol=\"type_label\")\n",
    "indexed_df = type_indexer.fit(silver_df_with_labels).transform(silver_df_with_labels)\n",
    "\n",
    "# NEW: Indexing Town_City for geographic context (Requirement 2a: Feature Engineering)\n",
    "city_indexer = StringIndexer(inputCol=\"Town_City\", outputCol=\"city_label\", handleInvalid=\"skip\")\n",
    "indexed_df = city_indexer.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "# B. Custom Segmentation\n",
    "segmenter = PriceSegmenter()\n",
    "segmented_df = segmenter.transform(indexed_df)\n",
    "\n",
    "# C. Multi-Stage Vector Assembly & StandardScaler\n",
    "# First, assemble the unscaled price\n",
    "price_assembler = VectorAssembler(inputCols=[\"Price\"], outputCol=\"unscaled_price\")\n",
    "price_assembled_df = price_assembler.transform(segmented_df)\n",
    "\n",
    "# Scale the price feature\n",
    "scaler = StandardScaler(inputCol=\"unscaled_price\", outputCol=\"scaled_features\", \n",
    "                        withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(price_assembled_df)\n",
    "scaled_df = scaler_model.transform(price_assembled_df)\n",
    "\n",
    "# NEW: Combine Scaled Price and City Label into the Final Feature Vector\n",
    "final_assembler = VectorAssembler(inputCols=[\"scaled_features\", \"city_label\"], outputCol=\"final_features\")\n",
    "final_engineered_df = final_assembler.transform(scaled_df)\n",
    "\n",
    "# ==========================================\n",
    "# 6. STORAGE & PERFORMANCE EVIDENCE (Requirement 1a & 1c)\n",
    "# ==========================================\n",
    "output_path = \"/Volumes/workspace/default/uk_land_registry/silver_engineered_parquet\"\n",
    "final_engineered_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# Stop the timer and print stats for Dashboard 4\n",
    "eng_duration = profiler.end_timer(\"Feature Engineering\")\n",
    "\n",
    "print(\"\\n--- PERFORMANCE SUMMARY ---\")\n",
    "print(f\"Feature Engineering Duration: {eng_duration:.2f} seconds\")\n",
    "\n",
    "# Explain Plan for optimization evidence\n",
    "final_engineered_df.explain(mode=\"formatted\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. GITHUB SAMPLE GENERATION (Local Meta-Data)\n",
    "# ==========================================\n",
    "sample_path = \"/Volumes/workspace/default/uk_land_registry/github_samples\"\n",
    "dbutils.fs.mkdirs(sample_path)\n",
    "\n",
    "final_engineered_df.limit(1000).toPandas().to_csv(f\"{sample_path}/silver_sample.csv\", index=False)\n",
    "print(f\"GitHub Silver sample generated in {sample_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_feature_engineering 2026-02-17 09:59:33",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}